{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KArimCHidekh/Bert-NLP/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "NEknCBG_TFM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MzYph_Knfx1",
        "outputId": "da5496b9-e24d-4fe9-87af-b7567b23fd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.2.0+cu92 in /usr/local/lib/python3.7/dist-packages (1.2.0+cu92)\n",
            "Requirement already satisfied: torchvision==0.4.0+cu92 in /usr/local/lib/python3.7/dist-packages (0.4.0+cu92)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.2.0+cu92) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.0+cu92) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.5.1 in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.0.53)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.12.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers==4.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA42iZUvnfvt"
      },
      "outputs": [],
      "source": [
        "data_complaint = pd.read_csv('complaint1700.csv')\n",
        "data_non_complaint = pd.read_csv('noncomplaint1700.csv')\n",
        "#tweets = pd.read_csv('Tweets.csv')\n",
        "\n",
        "#tweets.sample(5)\n",
        "#test_data2 = pd.read_csv('sentiment_tweets3.csv')\n",
        "\n",
        "#test_data2['label (depression result)'].value_counts()\n",
        "\n",
        "#list(test_data2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "vK26LjvUoln2",
        "outputId": "9fbcac21-0212-43ea-fa6e-16fd344b4701"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-460544fa-20b3-455b-b0b3-7446e2d356be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>airline</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2158</th>\n",
              "      <td>38377</td>\n",
              "      <td>American</td>\n",
              "      <td>@oldpathsjournal @AmericanAir You poor thing! ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331</th>\n",
              "      <td>101278</td>\n",
              "      <td>SouthWest</td>\n",
              "      <td>Hey @SouthwestAir, 128 planes with \"missed\" MX...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>31199</td>\n",
              "      <td>United</td>\n",
              "      <td>@united Friend called for change of flight. Un...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>39588</td>\n",
              "      <td>SouthWest</td>\n",
              "      <td>@SouthwestAir I still haven't heard back from ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839</th>\n",
              "      <td>15551</td>\n",
              "      <td>American</td>\n",
              "      <td>@laurahorn10 @AmericanAir Ooo she mad</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2529</th>\n",
              "      <td>77551</td>\n",
              "      <td>JetBlue</td>\n",
              "      <td>@JetBlue so yr message is \"next time we do a p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1273</th>\n",
              "      <td>64297</td>\n",
              "      <td>United</td>\n",
              "      <td>@united shame on you for leaving that dog out ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>8379</td>\n",
              "      <td>SouthWest</td>\n",
              "      <td>@Feelmyflow @SouthwestAir @AlaskaAir agreed, I...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3296</th>\n",
              "      <td>157938</td>\n",
              "      <td>JetBlue</td>\n",
              "      <td>@JetBlue handling compliments better than the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2252</th>\n",
              "      <td>47427</td>\n",
              "      <td>AlaskaAir</td>\n",
              "      <td>I find it ironic that we arrived at gate L8. B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>26914</td>\n",
              "      <td>VirginAmerica</td>\n",
              "      <td>You know what would be great @VirginAmerica? I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>68188</td>\n",
              "      <td>United</td>\n",
              "      <td>@united what's up with @fly2ohare ops? Waiting...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>148069</td>\n",
              "      <td>American</td>\n",
              "      <td>.@AmericanAirÃ¢â‚¬â„¢s Will Ris: We need to de...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2938</th>\n",
              "      <td>122869</td>\n",
              "      <td>American</td>\n",
              "      <td>@KellyAugustineB @AmericanAir wait what</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2416</th>\n",
              "      <td>66751</td>\n",
              "      <td>Delta</td>\n",
              "      <td>@DeltaAssist Is this real? 2 delay? Just kiddi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>153528</td>\n",
              "      <td>American</td>\n",
              "      <td>@AmericanAir @nickcapra quit screwing up his f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>125289</td>\n",
              "      <td>American</td>\n",
              "      <td>Watching a careless @AmericanAir employee drop...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3013</th>\n",
              "      <td>128821</td>\n",
              "      <td>American</td>\n",
              "      <td>@AmericanAir is there a loophole that would le...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2699</th>\n",
              "      <td>95294</td>\n",
              "      <td>VirginAmerica</td>\n",
              "      <td>@staringispolite @PhilzCoffee Wait, people fly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1790</th>\n",
              "      <td>10355</td>\n",
              "      <td>JetBlue</td>\n",
              "      <td>Best and Worst Airlines for #FlightDelays http...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-460544fa-20b3-455b-b0b3-7446e2d356be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-460544fa-20b3-455b-b0b3-7446e2d356be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-460544fa-20b3-455b-b0b3-7446e2d356be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          id        airline  \\\n",
              "2158   38377       American   \n",
              "1331  101278      SouthWest   \n",
              "508    31199         United   \n",
              "1085   39588      SouthWest   \n",
              "1839   15551       American   \n",
              "2529   77551        JetBlue   \n",
              "1273   64297         United   \n",
              "768     8379      SouthWest   \n",
              "3296  157938        JetBlue   \n",
              "2252   47427      AlaskaAir   \n",
              "2016   26914  VirginAmerica   \n",
              "862    68188         United   \n",
              "3173  148069       American   \n",
              "2938  122869       American   \n",
              "2416   66751          Delta   \n",
              "1376  153528       American   \n",
              "1542  125289       American   \n",
              "3013  128821       American   \n",
              "2699   95294  VirginAmerica   \n",
              "1790   10355        JetBlue   \n",
              "\n",
              "                                                  tweet  label  \n",
              "2158  @oldpathsjournal @AmericanAir You poor thing! ...      1  \n",
              "1331  Hey @SouthwestAir, 128 planes with \"missed\" MX...      0  \n",
              "508   @united Friend called for change of flight. Un...      0  \n",
              "1085  @SouthwestAir I still haven't heard back from ...      0  \n",
              "1839              @laurahorn10 @AmericanAir Ooo she mad      1  \n",
              "2529  @JetBlue so yr message is \"next time we do a p...      1  \n",
              "1273  @united shame on you for leaving that dog out ...      0  \n",
              "768   @Feelmyflow @SouthwestAir @AlaskaAir agreed, I...      0  \n",
              "3296  @JetBlue handling compliments better than the ...      1  \n",
              "2252  I find it ironic that we arrived at gate L8. B...      1  \n",
              "2016  You know what would be great @VirginAmerica? I...      1  \n",
              "862   @united what's up with @fly2ohare ops? Waiting...      0  \n",
              "3173  .@AmericanAirÃ¢â‚¬â„¢s Will Ris: We need to de...      1  \n",
              "2938            @KellyAugustineB @AmericanAir wait what      1  \n",
              "2416  @DeltaAssist Is this real? 2 delay? Just kiddi...      1  \n",
              "1376  @AmericanAir @nickcapra quit screwing up his f...      0  \n",
              "1542  Watching a careless @AmericanAir employee drop...      0  \n",
              "3013  @AmericanAir is there a loophole that would le...      1  \n",
              "2699  @staringispolite @PhilzCoffee Wait, people fly...      1  \n",
              "1790  Best and Worst Airlines for #FlightDelays http...      1  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_complaint['label'] = 0\n",
        "data_non_complaint['label'] = 1\n",
        "\n",
        "# Concatenate complaining and non-complaining data\n",
        "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Drop 'airline' column\n",
        "#data.drop(['airline'], inplace=True, axis=1)\n",
        "\n",
        "# Display 5 random samples\n",
        "data.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        },
        "id": "MhoHiW4CCp-T",
        "outputId": "2c4339be-b935-44ed-9948-0048c2515ba8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "11715  567802255215460353          negative                        1.0000   \n",
              "6140   568187725834289152           neutral                        1.0000   \n",
              "1727   569615215144226816          positive                        0.6497   \n",
              "9634   569760267485122560          negative                        1.0000   \n",
              "5653   568832907525152769          negative                        1.0000   \n",
              "2220   569309296023998464          negative                        1.0000   \n",
              "11780  567762685200228352           neutral                        0.6845   \n",
              "4815   569701794714128384          negative                        1.0000   \n",
              "2905   568835255722860544          positive                        1.0000   \n",
              "4907   569666470482804737          negative                        1.0000   \n",
              "\n",
              "               negativereason  negativereason_confidence     airline  \\\n",
              "11715  Customer Service Issue                     0.6498  US Airways   \n",
              "6140                      NaN                        NaN   Southwest   \n",
              "1727                      NaN                     0.0000      United   \n",
              "9634              Late Flight                     0.6660  US Airways   \n",
              "5653   Customer Service Issue                     0.6735   Southwest   \n",
              "2220                longlines                     0.6907      United   \n",
              "11780                     NaN                     0.0000  US Airways   \n",
              "4815   Customer Service Issue                     1.0000   Southwest   \n",
              "2905                      NaN                        NaN      United   \n",
              "4907               Can't Tell                     1.0000   Southwest   \n",
              "\n",
              "      airline_sentiment_gold          name negativereason_gold  retweet_count  \\\n",
              "11715                    NaN      Ltos5363                 NaN              0   \n",
              "6140                     NaN  Armywives101                 NaN              0   \n",
              "1727                     NaN   steve__chin                 NaN              0   \n",
              "9634                     NaN  JackieSeigel                 NaN              0   \n",
              "5653                     NaN       noli528                 NaN              0   \n",
              "2220                     NaN    ZachAlpert                 NaN              0   \n",
              "11780                    NaN    Eagles_SAB                 NaN              0   \n",
              "4815                     NaN    zarrylarou                 NaN              0   \n",
              "2905                     NaN       lcadler                 NaN              0   \n",
              "4907                     NaN       k_wasie                 NaN              0   \n",
              "\n",
              "                                                    text tweet_coord  \\\n",
              "11715  @USAirways  If you can waive a fee at time of ...         NaN   \n",
              "6140   @SouthwestAir Working on a piece about militar...         NaN   \n",
              "1727   @united thx off the response, finally got thro...         NaN   \n",
              "9634   @USAirways pls get me back to Tallahassee:( no...         NaN   \n",
              "5653   @SouthwestAir Flight to Nashville was Cancelle...         NaN   \n",
              "2220   @united instead of be told when we board, we h...         NaN   \n",
              "11780  @USAirways Darn it, first in line for no upgra...         NaN   \n",
              "4815   .@SouthwestAir glad you appreciate it, it’ll b...         NaN   \n",
              "2905   @united Baggage check in and in flight crew th...         NaN   \n",
              "4907   @SouthwestAir wifi is the worst $8 investment ...         NaN   \n",
              "\n",
              "                   tweet_created                tweet_location  \\\n",
              "11715  2015-02-17 13:46:21 -0800               South Shore, MA   \n",
              "6140   2015-02-18 15:18:04 -0800   Fayetteville North Carolina   \n",
              "1727   2015-02-22 13:50:24 -0800        San Francisco Bay Area   \n",
              "9634   2015-02-22 23:26:48 -0800                           NaN   \n",
              "5653   2015-02-20 10:01:48 -0800                 Nashville, TN   \n",
              "2220   2015-02-21 17:34:48 -0800                   Chicago, IL   \n",
              "11780  2015-02-17 11:09:07 -0800              Philadelphia, PA   \n",
              "4815   2015-02-22 19:34:27 -0800                            NY   \n",
              "2905   2015-02-20 10:11:08 -0800                           NaN   \n",
              "4907   2015-02-22 17:14:05 -0800  The happiest place on Earth!   \n",
              "\n",
              "                    user_timezone  \n",
              "11715  Eastern Time (US & Canada)  \n",
              "6140   Eastern Time (US & Canada)  \n",
              "1727   Pacific Time (US & Canada)  \n",
              "9634   Central Time (US & Canada)  \n",
              "5653   Central Time (US & Canada)  \n",
              "2220   Central Time (US & Canada)  \n",
              "11780  Central Time (US & Canada)  \n",
              "4815   Eastern Time (US & Canada)  \n",
              "2905   Pacific Time (US & Canada)  \n",
              "4907   Eastern Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cf550cd-3f0a-4846-b4c0-952cda41fa83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11715</th>\n",
              "      <td>567802255215460353</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>0.6498</td>\n",
              "      <td>US Airways</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Ltos5363</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@USAirways  If you can waive a fee at time of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-17 13:46:21 -0800</td>\n",
              "      <td>South Shore, MA</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6140</th>\n",
              "      <td>568187725834289152</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Southwest</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Armywives101</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@SouthwestAir Working on a piece about militar...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-18 15:18:04 -0800</td>\n",
              "      <td>Fayetteville North Carolina</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1727</th>\n",
              "      <td>569615215144226816</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.6497</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>United</td>\n",
              "      <td>NaN</td>\n",
              "      <td>steve__chin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@united thx off the response, finally got thro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 13:50:24 -0800</td>\n",
              "      <td>San Francisco Bay Area</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9634</th>\n",
              "      <td>569760267485122560</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Late Flight</td>\n",
              "      <td>0.6660</td>\n",
              "      <td>US Airways</td>\n",
              "      <td>NaN</td>\n",
              "      <td>JackieSeigel</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@USAirways pls get me back to Tallahassee:( no...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 23:26:48 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5653</th>\n",
              "      <td>568832907525152769</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>0.6735</td>\n",
              "      <td>Southwest</td>\n",
              "      <td>NaN</td>\n",
              "      <td>noli528</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@SouthwestAir Flight to Nashville was Cancelle...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-20 10:01:48 -0800</td>\n",
              "      <td>Nashville, TN</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>569309296023998464</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>longlines</td>\n",
              "      <td>0.6907</td>\n",
              "      <td>United</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ZachAlpert</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@united instead of be told when we board, we h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-21 17:34:48 -0800</td>\n",
              "      <td>Chicago, IL</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11780</th>\n",
              "      <td>567762685200228352</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6845</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>US Airways</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eagles_SAB</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@USAirways Darn it, first in line for no upgra...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-17 11:09:07 -0800</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4815</th>\n",
              "      <td>569701794714128384</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Southwest</td>\n",
              "      <td>NaN</td>\n",
              "      <td>zarrylarou</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>.@SouthwestAir glad you appreciate it, it’ll b...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 19:34:27 -0800</td>\n",
              "      <td>NY</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2905</th>\n",
              "      <td>568835255722860544</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>United</td>\n",
              "      <td>NaN</td>\n",
              "      <td>lcadler</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@united Baggage check in and in flight crew th...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-20 10:11:08 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4907</th>\n",
              "      <td>569666470482804737</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Southwest</td>\n",
              "      <td>NaN</td>\n",
              "      <td>k_wasie</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@SouthwestAir wifi is the worst $8 investment ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 17:14:05 -0800</td>\n",
              "      <td>The happiest place on Earth!</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cf550cd-3f0a-4846-b4c0-952cda41fa83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5cf550cd-3f0a-4846-b4c0-952cda41fa83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5cf550cd-3f0a-4846-b4c0-952cda41fa83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tweets = pd.read_csv('Tweets.csv')\n",
        "tweets.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = tweets"
      ],
      "metadata": {
        "id": "jIXMyZDbshOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgPya25Hnftn",
        "outputId": "3cc489df-5a39-4107-f78c-6c9d7c115047"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tweet_id', 'airline_sentiment', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "tweets = pd.read_csv('Tweets.csv')\n",
        "tweets.drop('airline', axis=1, inplace=True)\n",
        "tweets.drop('negativereason', axis=1, inplace=True)\n",
        "tweets.drop('negativereason_confidence', axis=1, inplace=True)\n",
        "tweets.drop('airline_sentiment_confidence', axis=1, inplace=True)\n",
        "\n",
        "tweets.drop('negativereason_gold', axis=1, inplace=True)\n",
        "tweets.drop('airline_sentiment_gold', axis=1, inplace=True)\n",
        "tweets.drop('retweet_count', axis=1, inplace=True)\n",
        "tweets.drop('tweet_coord', axis=1, inplace=True)\n",
        "\n",
        "tweets.drop('tweet_location', axis=1, inplace=True)\n",
        "tweets.drop('tweet_created', axis=1, inplace=True)\n",
        "tweets.drop('user_timezone', axis=1, inplace=True)\n",
        "tweets.drop('name', axis=1, inplace=True)\n",
        "#tweets.drop('airline', axis=1, inplace=True)\n",
        "\n",
        "#tweets.drop('airline', axis=2, inplace=True)\n",
        "\n",
        "#tweets.drop('',axis=1, inplace=True)\n",
        "\n",
        "# Check the current list of columns\n",
        "list(tweets.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2.rename(columns = {'text':'tweet'}, inplace = True)"
      ],
      "metadata": {
        "id": "DPh_44wVszAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67QzisSxtBQP",
        "outputId": "bce6bd3d-a23a-4428-b3b2-6a7db46f5a2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "tweets['airline_sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkUDtnF1nfrj"
      },
      "outputs": [],
      "source": [
        "newdf = data2.query('airline_sentiment == \"negative\" ')\n",
        "newdf2 = data2.query('airline_sentiment == \"positive\" ')\n",
        "newdf3 = data2.query('airline_sentiment == \"neutral\" ')\n",
        "\n",
        "\n",
        "#test_data2 = tweets.query('airline_sentiment == \"neutral\" ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newdf['label'] = 0\n",
        "newdf2['label'] = 1\n",
        "newdf3 ['label'] = 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7z7Y4XutbN2",
        "outputId": "561441c8-d02c-4e03-c9af-9ead9561b17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.concat([newdf, newdf2, newdf3], axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_sHyBQ6Utz7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "V1wMwf4jt9HS",
        "outputId": "bc584493-0f77-4c94-9aef-d0468913f89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 tweet_id airline_sentiment  \\\n",
              "671    569883984190005248          negative   \n",
              "13875  569961481455976449           neutral   \n",
              "14540  569698461911683073           neutral   \n",
              "2876   570257631778107392          negative   \n",
              "1182   569528348126216193          negative   \n",
              "\n",
              "                                                   tweet  label  \n",
              "671    @united 768 SFO to BOS and no internet it is 2...      0  \n",
              "13875  @USAirways is there an additional charge for a...      2  \n",
              "14540             @AmericanAir she doesn't have Twitter.      2  \n",
              "2876   @SouthwestAir mins and no answer. My wife and ...      0  \n",
              "1182   @united any chance you could help rebook?? My ...      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3fa1b362-4859-4641-967a-a76566783f1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>569883984190005248</td>\n",
              "      <td>negative</td>\n",
              "      <td>@united 768 SFO to BOS and no internet it is 2...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13875</th>\n",
              "      <td>569961481455976449</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@USAirways is there an additional charge for a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14540</th>\n",
              "      <td>569698461911683073</td>\n",
              "      <td>neutral</td>\n",
              "      <td>@AmericanAir she doesn't have Twitter.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2876</th>\n",
              "      <td>570257631778107392</td>\n",
              "      <td>negative</td>\n",
              "      <td>@SouthwestAir mins and no answer. My wife and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1182</th>\n",
              "      <td>569528348126216193</td>\n",
              "      <td>negative</td>\n",
              "      <td>@united any chance you could help rebook?? My ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fa1b362-4859-4641-967a-a76566783f1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3fa1b362-4859-4641-967a-a76566783f1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3fa1b362-4859-4641-967a-a76566783f1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "fFX0BwLwnfoG",
        "outputId": "f1f6425e-fbd8-4e68-f2c3-c03cd1c32cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b2f50b77-c81b-4afc-b2ab-49ae8eea74da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3367</th>\n",
              "      <td>569165378359656448</td>\n",
              "      <td>@SouthwestAir this is ridiculous. It's been 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1809</th>\n",
              "      <td>568905087608881152</td>\n",
              "      <td>@united Existing reservation is fine. I was ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>570276245960835073</td>\n",
              "      <td>@AmericanAir said that AA does not provide in-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1878</th>\n",
              "      <td>568848699960332288</td>\n",
              "      <td>@united what I don't understand isn't flight t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>569635623348215808</td>\n",
              "      <td>@united seriously what's with the slow #wifi o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2956</th>\n",
              "      <td>569977223198822400</td>\n",
              "      <td>@SouthwestAir like kelsey said, really bad spo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>569247769166266368</td>\n",
              "      <td>@VirginAmerica seats in Row 8 don't recline sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11353</th>\n",
              "      <td>570016890996666368</td>\n",
              "      <td>@AmericanAir Lady at B1 ABQ for 5347 on 2/23 w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>568793043664560128</td>\n",
              "      <td>@united 2nd time in a row  I've been over char...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>569438531686424577</td>\n",
              "      <td>@united of course I would like hepl! Are you k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5033</th>\n",
              "      <td>570231607757381632</td>\n",
              "      <td>@USAirways @AmericanAir our honeymoon was dela...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9224</th>\n",
              "      <td>569354267854704641</td>\n",
              "      <td>@VirginAmerica Thank you for the follow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5317</th>\n",
              "      <td>569928540004753409</td>\n",
              "      <td>@USAirways nice try at the gate making travele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6406</th>\n",
              "      <td>568891505869344770</td>\n",
              "      <td>@USAirways how about checking the plane before...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10126</th>\n",
              "      <td>568781747128803328</td>\n",
              "      <td>@SouthwestAir i live in the southwest, Imagine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5178</th>\n",
              "      <td>570015470570639360</td>\n",
              "      <td>@USAirways and despite tailwind… Will still be...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2f50b77-c81b-4afc-b2ab-49ae8eea74da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2f50b77-c81b-4afc-b2ab-49ae8eea74da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2f50b77-c81b-4afc-b2ab-49ae8eea74da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                 tweet_id                                              tweet\n",
              "3367   569165378359656448  @SouthwestAir this is ridiculous. It's been 2 ...\n",
              "1809   568905087608881152  @united Existing reservation is fine. I was ta...\n",
              "7375   570276245960835073  @AmericanAir said that AA does not provide in-...\n",
              "1878   568848699960332288  @united what I don't understand isn't flight t...\n",
              "995    569635623348215808  @united seriously what's with the slow #wifi o...\n",
              "2956   569977223198822400  @SouthwestAir like kelsey said, really bad spo...\n",
              "84     569247769166266368  @VirginAmerica seats in Row 8 don't recline sh...\n",
              "11353  570016890996666368  @AmericanAir Lady at B1 ABQ for 5347 on 2/23 w...\n",
              "1964   568793043664560128  @united 2nd time in a row  I've been over char...\n",
              "1287   569438531686424577  @united of course I would like hepl! Are you k...\n",
              "5033   570231607757381632  @USAirways @AmericanAir our honeymoon was dela...\n",
              "9224   569354267854704641            @VirginAmerica Thank you for the follow\n",
              "5317   569928540004753409  @USAirways nice try at the gate making travele...\n",
              "6406   568891505869344770  @USAirways how about checking the plane before...\n",
              "10126  568781747128803328  @SouthwestAir i live in the southwest, Imagine...\n",
              "5178   570015470570639360  @USAirways and despite tailwind… Will still be..."
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "newdf = tweets.query('airline_sentiment == \"negative\" ')\n",
        "newdf2 = tweets.query('airline_sentiment == \"positive\" ')\n",
        "test_data = pd.concat([newdf, newdf2], axis=0).reset_index(drop=True)\n",
        "\n",
        "test_data.drop('airline_sentiment', axis=1, inplace=True)\n",
        "test_data.rename(columns = {'text':'tweet'}, inplace = True)\n",
        "test_data.sample(16)\n",
        "\n",
        "#test_data['airline_sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ng6rVnSmsMzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnR0Q6CPnfkU",
        "outputId": "9537b445-657f-40cf-f02e-830abf1bf64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re\n",
        "# Uncomment to download \"stopwords\"\n",
        "# nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \n",
        "    s = s.lower()\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Remove @name\n",
        "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    # s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = re.sub(r'@[A-Za-z0-9]+','',s)\n",
        "    s = re.sub(r'#' , '',s)\n",
        "    s= re.sub(r'RT[\\s]+','',s)\n",
        "    s = re.sub(r'https?:\\/\\/\\S+','',s)\n",
        "    s = re.sub(r'&amp;', '&', s)\n",
        "\n",
        "    # remove numbers\n",
        "    s = re.sub(r'\\d+', '', s)\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCGAg256nfiH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data2.tweet.values\n",
        "y = data2.label.values\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X, y, test_size=0.2, random_state=2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCE4P9QnfeE",
        "outputId": "f3fc9635-16af-445e-e5c7-906623ff4292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  @VirginAmerica SFO-PDX schedule is still MIA.\n",
            "Processed:  sfo-pdx schedule still mia .\n"
          ]
        }
      ],
      "source": [
        "# Print sentence 0\n",
        "print('Original: ', X[3])\n",
        "print('Processed: ', text_preprocessing(X[3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmkS9r6fnfbm"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(dt):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in dt:\n",
        "        \n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  \n",
        "            add_special_tokens=True,        \n",
        "            max_length=MAX_LEN,                  \n",
        "            pad_to_max_length=True,         \n",
        "            #return_tensors='pt',           \n",
        "            return_attention_mask=True      \n",
        "            )\n",
        "        \n",
        "      \n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "   \n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRBsFB00nfZU",
        "outputId": "00084124-a013-46cc-e6b2-bd1bd8f4d40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length:  67\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Concatenate train data and test data\n",
        "all_tweets = data2.tweet.values\n",
        "\n",
        "# Encode our concatenated data\n",
        "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "\n",
        "# Find the maximum length\n",
        "MAX_LEN = max([len(sent) for sent in encoded_tweets])\n",
        "print('Max length: ', MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qt2Y6ceX2qe"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 68"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6uBz4rmpBKy",
        "outputId": "113c2156-0b68-47ae-d8c0-b50a3d01704e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  @VirginAmerica and it's a really big bad thing about it\n",
            "Token IDs:  [101, 1005, 2428, 2502, 2919, 2518, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Original:  @VirginAmerica your airline is awesome but your lax loft needs to step up its game. $40 for dirty tables and floors? http://t.co/hy0VrfhjHt\n",
            "Token IDs:  [101, 8582, 12476, 27327, 19459, 3791, 3357, 2208, 1012, 1002, 6530, 7251, 8158, 1029, 8299, 1013, 1013, 1012, 2522, 1013, 1044, 2100, 19716, 2546, 2232, 3501, 11039, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Original:  @united @AmericanAir so that's it? It just ends there? Come on! I traveled for literally an extra day and a half because of this!\n",
            "Token IDs:  [101, 1005, 1029, 4515, 1029, 2272, 999, 6158, 6719, 4469, 2154, 2431, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Original:  @united your worker told us not to call because we'd be talking to someone in Thailand...\n",
            "Token IDs:  [101, 7309, 2409, 2149, 2025, 2655, 1005, 3331, 2619, 6504, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ]
        }
      ],
      "source": [
        "# Print sentence 0 and its encoded token ids\n",
        "\n",
        "\n",
        "token_ids = list(preprocessing_for_bert([X[1]])[0].squeeze().numpy())\n",
        "print('Original: ', X[1])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "token_ids = list(preprocessing_for_bert([X[20]])[0].squeeze().numpy())\n",
        "print('Original: ', X[20])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "token_ids = list(preprocessing_for_bert([X[400]])[0].squeeze().numpy())\n",
        "print('Original: ', X[400])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "token_ids = list(preprocessing_for_bert([X[852]])[0].squeeze().numpy())\n",
        "print('Original: ', X[852])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZlGajm9pFLm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns0RtFskpJSu",
        "outputId": "c9ad1461-ee6b-4da1-92fd-8f826be99ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 59 µs, sys: 0 ns, total: 59 µs\n",
            "Wall time: 63.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 3\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo8kJ3ffpiT2"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs \n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vszu2Aq-ppNZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5pvrLMgpr_-",
        "outputId": "21ce6e25-b2e0-4d77-fed7-91e19a2fc26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFDxw2tqptM3",
        "outputId": "173b088a-274b-4934-b47b-4fc65378d630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.799580   |     -      |     -     |   9.24   \n",
            "   1    |   40    |   0.638954   |     -      |     -     |   8.92   \n",
            "   1    |   60    |   0.580435   |     -      |     -     |   9.03   \n",
            "   1    |   80    |   0.510927   |     -      |     -     |   8.97   \n",
            "   1    |   100   |   0.541549   |     -      |     -     |   8.85   \n",
            "   1    |   120   |   0.497838   |     -      |     -     |   8.77   \n",
            "   1    |   140   |   0.544525   |     -      |     -     |   8.73   \n",
            "   1    |   160   |   0.507932   |     -      |     -     |   8.70   \n",
            "   1    |   180   |   0.443446   |     -      |     -     |   8.83   \n",
            "   1    |   200   |   0.529255   |     -      |     -     |   8.98   \n",
            "   1    |   220   |   0.475977   |     -      |     -     |   8.79   \n",
            "   1    |   240   |   0.534094   |     -      |     -     |   8.83   \n",
            "   1    |   260   |   0.493070   |     -      |     -     |   8.85   \n",
            "   1    |   280   |   0.442132   |     -      |     -     |   8.84   \n",
            "   1    |   300   |   0.511332   |     -      |     -     |   8.83   \n",
            "   1    |   320   |   0.479920   |     -      |     -     |   8.79   \n",
            "   1    |   340   |   0.457295   |     -      |     -     |   8.80   \n",
            "   1    |   360   |   0.432634   |     -      |     -     |   8.79   \n",
            "   1    |   365   |   0.502068   |     -      |     -     |   2.19   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.523846   |  0.446768  |   82.20   |  175.37  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.334133   |     -      |     -     |   9.20   \n",
            "   2    |   40    |   0.297310   |     -      |     -     |   8.80   \n",
            "   2    |   60    |   0.309176   |     -      |     -     |   8.80   \n",
            "   2    |   80    |   0.291947   |     -      |     -     |   8.80   \n",
            "   2    |   100   |   0.299252   |     -      |     -     |   8.80   \n",
            "   2    |   120   |   0.319241   |     -      |     -     |   8.80   \n",
            "   2    |   140   |   0.321329   |     -      |     -     |   8.77   \n",
            "   2    |   160   |   0.334846   |     -      |     -     |   8.78   \n",
            "   2    |   180   |   0.334316   |     -      |     -     |   8.79   \n",
            "   2    |   200   |   0.304943   |     -      |     -     |   8.78   \n",
            "   2    |   220   |   0.306190   |     -      |     -     |   8.80   \n",
            "   2    |   240   |   0.277322   |     -      |     -     |   8.78   \n",
            "   2    |   260   |   0.308882   |     -      |     -     |   8.80   \n",
            "   2    |   280   |   0.271410   |     -      |     -     |   8.80   \n",
            "   2    |   300   |   0.294863   |     -      |     -     |   8.82   \n",
            "   2    |   320   |   0.290455   |     -      |     -     |   8.80   \n",
            "   2    |   340   |   0.326752   |     -      |     -     |   8.81   \n",
            "   2    |   360   |   0.343860   |     -      |     -     |   8.79   \n",
            "   2    |   365   |   0.385157   |     -      |     -     |   2.20   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.310340   |  0.472119  |   82.98   |  174.59  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.random import set_seed \n",
        "\n",
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "nf = train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MzUkMJcp1c3"
      },
      "outputs": [],
      "source": [
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = [] \n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab_gz5MFp4b5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUvyAW9yp7DP"
      },
      "outputs": [],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSSKCQ5lp9zv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "ss1WHkA2qCEQ",
        "outputId": "cc742a1d-00c4-47a9-a1fa-6c5404e276eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.8881\n",
            "Accuracy: 77.35%\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "evaluate_roc(probs, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35YUO8OkqFKv"
      },
      "outputs": [],
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OgQQneIqHnP",
        "outputId": "1e5697df-47a6-4d66-ee54-4d99b20140fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.673726   |     -      |     -     |   7.36   \n",
            "   1    |   40    |   0.551986   |     -      |     -     |   7.06   \n",
            "   1    |   60    |   0.518128   |     -      |     -     |   7.07   \n",
            "   1    |   80    |   0.550541   |     -      |     -     |   7.14   \n",
            "   1    |   100   |   0.443649   |     -      |     -     |   7.18   \n",
            "   1    |   106   |   0.431475   |     -      |     -     |   1.92   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.323710   |     -      |     -     |   7.61   \n",
            "   2    |   40    |   0.293927   |     -      |     -     |   7.25   \n",
            "   2    |   60    |   0.306363   |     -      |     -     |   7.25   \n",
            "   2    |   80    |   0.227043   |     -      |     -     |   7.30   \n",
            "   2    |   100   |   0.291181   |     -      |     -     |   7.31   \n",
            "   2    |   106   |   0.367705   |     -      |     -     |   1.96   \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, full_train_dataloader, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-JuUeG5qPVo",
        "outputId": "bd4a1a68-085c-4504-a739-faf8d3f59d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Run `preprocessing_for_bert` on the test set\n",
        "print('Tokenizing data...')\n",
        "test_inputs, test_masks = preprocessing_for_bert(test_data.tweet)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_dataset = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MGHsDi_1qSTh",
        "outputId": "a92a0c3a-0952-4b8e-b5f3-1917470a87f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "le nombre des tweets prédits positive:  2371\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "threshold = 0.944\n",
        "postive_prediction = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "#negative_prediction = np.where(probs[:, 1] > threshold, 0, 1)\n",
        "\n",
        "print(\"le nombre des tweets prédits positive: \", postive_prediction.sum())\n",
        "#print(\"Le nombre des tweets prédits négative: \", negative_prediction.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Dsx2-BqUDZ",
        "outputId": "28077def-d74a-45ae-fc20-f336c2300380"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['@AmericanAir thanks for getting back to me. how frequently are these weather advisories updated throughout the day?',\n",
              " '@SouthwestAir Have a cup coffee and relax while you check out the New Deals and Promotions at Avon, twice a month at Doug @dcoadavon',\n",
              " '@united can you please tell me where United flies to from Sydney Australia - pref Asia Pac or US as I have a voucher to redeem.',\n",
              " '@JetBlue Thank you ! What about Paris ? Could we arrange something from there ?',\n",
              " '@SouthwestAir yes, thank you. Just sent DM.',\n",
              " '@SouthwestAir better travel photos:\\nMy Kindle e-book Easy Tips guide http://t.co/7dM2J8H97M:\\nhttp://t.co/xeDeckGMW5 http://t.co/frGhglMkqF',\n",
              " '@VirginAmerica Now, when will we see VirginAmerica come to Philadelphia (PHL).',\n",
              " \"@AmericanAir We've sent you more info via DM.  I truly hope you resolve this very quickly. #media #filmcrew #cnn #nbc\",\n",
              " \"@JetBlue you're taking me to New Orleans in two weeks so I'll picture that while I shiver #forevercold\",\n",
              " '@JetBlue Thanks! I just sent a few DMs.',\n",
              " '@SouthwestAir New marketing song? https://t.co/F2LFULCbQ7 let us know what you think? http://t.co/iQnVyFPg4P',\n",
              " \"@SouthwestAir BTW, not a weather delay. We've had beautiful weather in Sunny California. #nolove #noexcuses #cali http://t.co/kumtbgER03\",\n",
              " '@united hi! I just bought a flight with united through ebokeers from Paris to Miami. Do I have any chance to check that everything is ok?',\n",
              " '@AmericanAir New marketing song? https://t.co/F2LFULCbQ7 let us know what you think? http://t.co/NBZkhiZF3V',\n",
              " '@united all day travel. #swag #ijustwanttosleep',\n",
              " \"@JetBlue Indeed. I don't know what's going on in Pittsburgh that weekend, but, it's drawing a crowd via JetBlue!\",\n",
              " '@united how much does it cost to check in an additional bag? Traveling from newark to glasgow. Thank you :)',\n",
              " 'Sorry, @united. I accidentally popuLate Flightd with you instead of @USAirways. Can\\'t wait \\'til \"U\" are the only \"U\" in the sky. #NewAmericanStinks',\n",
              " '@united flying out of STL Saturday where they are expecting ice &amp; snow. When should we start looking for travel waivers? Thanks!',\n",
              " \"@JetBlue Let's just say #IDontWannaLiveWithoutYourLove https://t.co/i9KCgaxxFa #ItWasMintToBe #BestInClassSocial #ThankYou #Travel #Business\"]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = test_data2[postive_prediction==1]\n",
        "list(output.sample(20).text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmjY0xXSw17Iy0HkKmNd+1",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}